{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from raise_utils.learners.learner import Learner\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding, SpatialDropout1D\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest(Learner):\n",
    "    \"\"\"Random forest classifier\"\"\"\n",
    "    def __init__(self, weighted=False, *args, **kwargs):\n",
    "        \"\"\"Initializes the classifier.\"\"\"\n",
    "        super(RandomForest, self).__init__(*args, **kwargs)\n",
    "\n",
    "        if weighted:\n",
    "            self.learner = RandomForestClassifier(class_weight=\"balanced\")\n",
    "        else:\n",
    "            self.learner = RandomForestClassifier()\n",
    "        self.random_map = {\n",
    "            \"criterion\": [\"gini\", \"entropy\"],\n",
    "            \"n_estimators\": (10, 100)\n",
    "        }\n",
    "        self._instantiate_random_vals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BILSTM(Learner):\n",
    "    def __init__(self, epochs=10, max_words=1000, embedding=5,n_layers=1, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Initializes the BILSTM Classifier.\n",
    "        :param epochs: Number of epochs to train for\n",
    "        :param max_words: Maximum number of top words to consider\n",
    "        :param embedding: Embedding dimensionality\n",
    "        :param n_layers: Number of LSTM layers\n",
    "        :param args: Args passed to Learner\n",
    "        :param kwargs: Keyword args passed to Learner\n",
    "        \"\"\"\n",
    "        super(BILSTM, self).__init__(*args, **kwargs)\n",
    "        self.epochs = epochs\n",
    "        self.max_words = max_words\n",
    "        self.embed_dim = embedding\n",
    "        self.n_layers = n_layers\n",
    "        # This is where we set our random attributes\n",
    "        self.random_map = {\n",
    "            \"max_words\": (500, 5000),\n",
    "            \"n_layers\": (1, 4)\n",
    "        }\n",
    "        self.learner = self\n",
    "        self._instantiate_random_vals()\n",
    "\n",
    "    def fit(self):\n",
    "        self._check_data()\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(self.max_words, self.embed_dim,\n",
    "                            input_length=self.x_train.shape[1]))\n",
    "        model.add(SpatialDropout1D(0.2))\n",
    "        for _ in range(self.n_layers):\n",
    "            model.add(Bidirectional(LSTM(100, dropout=0.2, recurrent_dropout=0.2)))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "        self.learner = model\n",
    "\n",
    "        if self.hooks is not None:\n",
    "            if self.hooks.get('pre_train', None):\n",
    "                for hook in self.hooks['pre_train']:\n",
    "                    hook.call(self)\n",
    "\n",
    "        model.fit(self.x_train, self.y_train,\n",
    "                  batch_size=64, epochs=self.epochs)\n",
    "\n",
    "        if self.hooks is not None:\n",
    "            if self.hooks.get('post_train', None):\n",
    "                for hook in self.hooks['post_train']:\n",
    "                    hook.call(model)\n",
    "\n",
    "    def predict_on_test(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Makes predictions\n",
    "        :param x_test: Test data\n",
    "        :return: np.ndarray\n",
    "        \"\"\"\n",
    "        return self.learner.predict_classes(self.x_test)\n",
    "\n",
    "    def predict(self, x_test):\n",
    "        \"\"\"\n",
    "        Overrides parent method, ignoring argument passed.\n",
    "        :param x_test: Ignored.\n",
    "        :return: Array of preds.\n",
    "        \"\"\"\n",
    "        warnings.warn(\"predict() should not be used with TextDeepLearner. Instead, use predict_on_test\" +\n",
    "                      \". The argument is ignored\")\n",
    "        return self.predict_on_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
